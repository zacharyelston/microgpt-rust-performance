MicroGPT: 4192 params, training for 3000 steps (lr=0.005, emb=16, head=4, layer=1)
step    0 | loss 3.3432step   20 | loss 2.9224step   40 | loss 2.3949step   60 | loss 2.3262step   80 | loss 2.7657step  100 | loss 2.8416step  120 | loss 2.6538step  140 | loss 2.5202step  160 | loss 2.2777step  180 | loss 2.3296step  200 | loss 2.9795step  220 | loss 1.9275step  240 | loss 2.8516step  260 | loss 2.4804step  280 | loss 3.2550step  300 | loss 1.6372step  320 | loss 1.7513step  340 | loss 2.3983step  360 | loss 2.7977step  380 | loss 2.0175step  400 | loss 2.3410step  420 | loss 2.0467step  440 | loss 2.4164step  460 | loss 2.3791step  480 | loss 2.7974step  500 | loss 1.8288step  520 | loss 2.3490step  540 | loss 1.7839step  560 | loss 1.7110step  580 | loss 1.8573step  600 | loss 2.5205step  620 | loss 3.0417step  640 | loss 2.5618step  660 | loss 3.5163step  680 | loss 1.5220step  700 | loss 1.6680step  720 | loss 1.8411step  740 | loss 1.4992step  760 | loss 2.5032step  780 | loss 1.8382step  800 | loss 2.1138step  820 | loss 1.9947step  840 | loss 2.1043step  860 | loss 2.0280step  880 | loss 2.3429step  900 | loss 3.3338step  920 | loss 1.9517step  940 | loss 1.9273step  960 | loss 2.1449step  980 | loss 2.4976step 1000 | loss 2.2611step 1020 | loss 2.3816step 1040 | loss 2.5718step 1060 | loss 2.4413step 1080 | loss 1.7510step 1100 | loss 1.9294step 1120 | loss 1.8855step 1140 | loss 2.2625step 1160 | loss 2.1477step 1180 | loss 2.1092step 1200 | loss 2.2178step 1220 | loss 3.0054step 1240 | loss 1.7602step 1260 | loss 1.8491step 1280 | loss 2.4600step 1300 | loss 2.1435step 1320 | loss 2.5928step 1340 | loss 2.0473step 1360 | loss 2.6929step 1380 | loss 2.3956step 1400 | loss 1.8852step 1420 | loss 2.4245step 1440 | loss 1.8472step 1460 | loss 1.7871step 1480 | loss 2.1309step 1500 | loss 1.6654step 1520 | loss 2.3040step 1540 | loss 2.1722step 1560 | loss 2.0972step 1580 | loss 1.6036step 1600 | loss 2.7469step 1620 | loss 2.0111step 1640 | loss 2.4665step 1660 | loss 2.3422step 1680 | loss 1.6831step 1700 | loss 2.6466step 1720 | loss 2.1641step 1740 | loss 1.8074step 1760 | loss 2.3589step 1780 | loss 2.6596step 1800 | loss 2.6214step 1820 | loss 1.8854step 1840 | loss 2.0973step 1860 | loss 1.5880step 1880 | loss 2.4155step 1900 | loss 3.2482step 1920 | loss 2.2097step 1940 | loss 1.6021step 1960 | loss 2.7834step 1980 | loss 2.0214step 2000 | loss 2.5305step 2020 | loss 2.2278step 2040 | loss 2.2738step 2060 | loss 1.8430step 2080 | loss 3.0450step 2100 | loss 1.6694step 2120 | loss 2.2182step 2140 | loss 2.3700step 2160 | loss 1.7159step 2180 | loss 3.1976step 2200 | loss 2.7549step 2220 | loss 2.4579step 2240 | loss 2.5580step 2260 | loss 1.9277step 2280 | loss 2.0211step 2300 | loss 1.9205step 2320 | loss 2.1192step 2340 | loss 1.8119step 2360 | loss 1.9892step 2380 | loss 1.7545step 2400 | loss 3.6094step 2420 | loss 2.4121step 2440 | loss 1.6830step 2460 | loss 2.0202step 2480 | loss 1.9040step 2500 | loss 2.2528step 2520 | loss 2.4711step 2540 | loss 1.9362step 2560 | loss 2.0857step 2580 | loss 1.5280step 2600 | loss 2.3041step 2620 | loss 2.0455step 2640 | loss 2.1755step 2660 | loss 2.1802step 2680 | loss 2.2072step 2700 | loss 2.0119step 2720 | loss 2.0648step 2740 | loss 2.4741step 2760 | loss 1.5754step 2780 | loss 1.8534step 2800 | loss 2.8928step 2820 | loss 2.1489step 2840 | loss 2.3822step 2860 | loss 1.8469step 2880 | loss 2.6364step 2900 | loss 2.1212step 2920 | loss 1.9162step 2940 | loss 1.7286step 2960 | loss 1.8971step 2980 | loss 2.4268
--- Generation ---
> adotely
> alaleesa
> ivela
> myina
> jumil
